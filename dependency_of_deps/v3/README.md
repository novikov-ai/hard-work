# Избавляемся от зависимости от зависимостей-3

## 1. Зависимость фреймворка

**Проблема:**  
В одном из проектов на Go мы использовали сторонний HTTP-фреймворк для маршрутизации запросов, но с каждым обновлением версия фреймворка ломала обратную совместимость, что вызывало падение сервиса при сборке.

**Решение:**  
Отказались от использования стороннего фреймворка и заменили его на стандартную библиотеку Go для HTTP-маршрутизации. Это дало большую гибкость и избавило нас от зависимости на обновления внешнего фреймворка, а также улучшило тестируемость кода.

## 2. Зависимость расшаренного формата

**Проблема:**  
При взаимодействии нескольких микросервисов мы использовали корпоративный стандарт похожий на Google Protocol Buffers для сериализации данных. Проблема возникла, когда одна команда обновила структуру данных без уведомления других команд, что привело к несовместимости сообщений между сервисами.

**Решение:**  
Мы внедрили строгий контроль версий схем и добавили автоматические тесты на совместимость, которые проверяют, что новые изменения не ломают существующие интеграции. Это исключило подобные проблемы в будущем.

## 3. Зависимость зависимости

**Проблема:**  
В микросервисе для авторизации мы использовали библиотеку для работы с токенами OAuth. В какой-то момент библиотека начала использовать новый алгоритм шифрования токенов, что нарушило совместимость с внешними сервисами, которые ожидали старый формат.

**Решение:**  
Мы создали адаптер, который временно поддерживал оба формата токенов, что позволило плавно перейти на новый алгоритм без нарушений работы внешних сервисов. В дальнейшем мы полностью переключились на новый формат после уведомления всех внешних систем.

## 4. Зависимость краша

**Проблема:**  
Один из наших микросервисов зависел от сторонней базы данных, и при её недоступности микросервис падал, что приводило к полному отказу части функциональности платформы.

**Решение:**  
Мы внедрили стратегию graceful degradation — при недоступности базы данных микросервис продолжает работать в режиме кэша, используя последнюю доступную информацию, а также добавили механизм ретраев для восстановления соединения с базой данных.

## 5. Зависимость перебрасывания

**Проблема:**  
В нашем проекте мы использовали сторонний API для обработки платежей. При сбоях этого API мы временно перебрасывали платежи на альтернативного провайдера, но он не поддерживал все функции первого, что вызывало ошибки в обработке платежей.

**Решение:**  
Мы изменили архитектуру таким образом, чтобы сервис корректно проверял поддерживаемые функции резервного провайдера до переброса, и в случае отсутствия необходимых функций уведомлял пользователя о невозможности выполнить платёж через резервного провайдера.

## 6. Зависимость инверсии

**Проблема:**  
Мы использовали библиотеку для логирования, которая встраивала свои зависимости в основной поток выполнения приложения. Это создавало риски непредсказуемого поведения при работе с логами, особенно в продакшн-окружении.

**Решение:**  
Мы внедрили интерфейсы для работы с логированием и создали собственные реализации, которые обеспечивают независимость от конкретных библиотек. Это позволило нам легко заменять решения для логирования без изменения основной логики приложения.

## 7. Зависимость зацикливания

**Проблема:**  
В одном из микросервисов возникла сложная зависимость: сервис A зависел от данных, предоставляемых сервисом B, который в свою очередь зависел от данных сервиса A. Это приводило к deadlock-ситуациям.

**Решение:**  
Мы реорганизовали архитектуру взаимодействия микросервисов, внедрив центральное хранилище для данных, которое позволяло обоим сервисам независимо запрашивать и обновлять данные, что исключило циклические зависимости.

## 8. Зависимость высшего порядка

**Проблема:**  
В одном из сервисов использовались сторонние API для получения данных, но эти API не гарантировали стабильности результатов, что приводило к тому, что некорректные данные влияли на качество работы нашего сервиса.

**Решение:**  
Мы внедрили систему валидации входных данных и fallback-логику, которая использует кэшированные данные или обращается к другому провайдеру в случае недостоверной информации от основного API. Это уменьшило влияние нестабильных данных на работу нашего сервиса.

## 9. Зависимость большинства

**Проблема:**  
Мы строили распределённую систему для обработки данных, где решения принимались на основе большинства узлов. Проблема возникла, когда некоторые узлы выходили из строя, что нарушало консенсус.

**Решение:**  
Мы реализовали алгоритм Raft для достижения консенсуса, который устойчив к отказу некоторых узлов и позволяет системе продолжать работу, если большинство узлов остаются доступными. Это обеспечило надёжность и отказоустойчивость системы. 